{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with PyTorch and AG_NEWS Dataset\n",
    "\n",
    "This notebook demonstrates document classification using PyTorch with gradient descent on the AG_NEWS dataset. The AG_NEWS dataset contains news articles from 4 categories: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for text classification with AG_NEWS data\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], vocab: Dict[str, int], tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "        return torch.LongTensor(indices), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    \"\"\"Neural network for document classification using embeddings and fully connected layers\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int, num_classes: int, dropout_rate: float = 0.3):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed tokens and pool by averaging\n",
    "        embedded = self.embedding(x)\n",
    "        pooled = torch.mean(embedded, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"Collate function for DataLoader to handle variable-length sequences\"\"\"\n",
    "    label_list, text_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label)\n",
    "        text_list.append(text)\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassificationPipeline:\n",
    "    \"\"\"Complete pipeline for document classification with AG_NEWS dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 10000, embed_dim: int = 64, hidden_size: int = 256, dropout_rate: float = 0.3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.label_names = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "        \n",
    "    def build_vocab(self, texts: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Build vocabulary from training texts\"\"\"\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        vocab = {'<pad>': 0, '<unk>': 1}\n",
    "        for i, (word, _) in enumerate(counter.most_common(self.vocab_size - 2)):\n",
    "            vocab[word] = i + 2\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def load_ag_news_data(self) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "        \"\"\"Load and preprocess AG_NEWS dataset\"\"\"\n",
    "        train_iter = AG_NEWS(split='train')\n",
    "        test_iter = AG_NEWS(split='test')\n",
    "        \n",
    "        train_texts, train_labels = [], []\n",
    "        for label, text in train_iter:\n",
    "            train_texts.append(text)\n",
    "            train_labels.append(label - 1)  # Convert to 0-indexed\n",
    "        \n",
    "        test_texts, test_labels = [], []\n",
    "        for label, text in test_iter:\n",
    "            test_texts.append(text)\n",
    "            test_labels.append(label - 1)  # Convert to 0-indexed\n",
    "        \n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "    \n",
    "    def create_model(self, num_classes: int):\n",
    "        \"\"\"Initialize the neural network model\"\"\"\n",
    "        self.model = DocumentClassifier(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        ).to(self.device)\n",
    "        \n",
    "    def train(self, learning_rate: float = 0.001, epochs: int = 10, batch_size: int = 64):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        print(\"Loading AG_NEWS dataset...\")\n",
    "        train_texts, train_labels, test_texts, test_labels = self.load_ag_news_data()\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(train_texts)} train samples, {len(test_texts)} test samples\")\n",
    "        \n",
    "        print(\"Building vocabulary...\")\n",
    "        self.vocab = self.build_vocab(train_texts)\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        train_dataset = DocumentDataset(train_texts, train_labels, self.vocab, self.tokenizer)\n",
    "        test_dataset = DocumentDataset(test_texts, test_labels, self.vocab, self.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "        \n",
    "        num_classes = 4\n",
    "        self.create_model(num_classes)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for batch_texts, batch_labels in train_loader:\n",
    "                batch_texts = batch_texts.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "                \n",
    "                # Gradient descent step\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += batch_labels.size(0)\n",
    "                correct_train += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "            \n",
    "            test_accuracy = self.evaluate(test_loader)\n",
    "            \n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "                  f'Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, test_loader: DataLoader) -> float:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in test_loader:\n",
    "                batch_texts = batch_texts.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(batch_texts)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        return 100 * correct / total\n",
    "    \n",
    "    def predict(self, documents: List[str]) -> List[str]:\n",
    "        \"\"\"Make predictions on new documents\"\"\"\n",
    "        if self.model is None or self.vocab is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in documents:\n",
    "                tokens = self.tokenizer(text)\n",
    "                indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "                text_tensor = torch.LongTensor(indices).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                output = self.model(text_tensor)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predicted_class = predicted.item() + 1\n",
    "                predictions.append(self.label_names[predicted_class])\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = DocumentClassificationPipeline(\n",
    "    vocab_size=20000, \n",
    "    embed_dim=100, \n",
    "    hidden_size=256\n",
    ")\n",
    "\n",
    "print(\"Training document classifier with AG_NEWS dataset...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = pipeline.train(\n",
    "    learning_rate=0.001,\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(results['train_losses'], label='Training Loss', color='blue')\n",
    "ax1.set_title('Training Loss Over Epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(results['train_accuracies'], label='Training Accuracy', color='green')\n",
    "ax2.plot(results['test_accuracies'], label='Test Accuracy', color='red')\n",
    "ax2.set_title('Accuracy Over Epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {results['train_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {results['test_accuracies'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample documents\n",
    "test_documents = [\n",
    "    \"Apple Inc. reported strong quarterly earnings beating analyst expectations.\",\n",
    "    \"NASA discovers water on Mars in groundbreaking space mission.\",\n",
    "    \"Lakers win NBA championship after defeating Celtics in game 7.\",\n",
    "    \"New iPhone features advanced AI chip for faster processing.\",\n",
    "    \"Stock market reaches new all-time high as investors remain optimistic.\",\n",
    "    \"Scientists develop breakthrough gene therapy for treating cancer.\",\n",
    "    \"World Cup final draws record television audience worldwide.\",\n",
    "    \"Tech companies face new regulations on data privacy and security.\"\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_documents)\n",
    "\n",
    "print(\"Predictions on sample texts:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, pred) in enumerate(zip(test_documents, predictions), 1):\n",
    "    print(f\"{i}. Text: {doc}\")\n",
    "    print(f\"   Predicted class: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cell for custom predictions\n",
    "def predict_custom_text(text: str):\n",
    "    \"\"\"Helper function to predict class for custom text\"\"\"\n",
    "    prediction = pipeline.predict([text])[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted class: {prediction}\")\n",
    "    return prediction\n",
    "\n",
    "# Example usage - modify the text below to test your own examples\n",
    "custom_text = \"The company's stock price soared after announcing record profits.\"\n",
    "predict_custom_text(custom_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 40)\n",
    "print(pipeline.model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in pipeline.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in pipeline.model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Vocabulary size: {len(pipeline.vocab):,}\")\n",
    "print(f\"Embedding dimension: {pipeline.embed_dim}\")\n",
    "print(f\"Hidden layer size: {pipeline.hidden_size}\")\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"Classes: {list(pipeline.label_names.values())}\")\n",
    "print(f\"Device used: {pipeline.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading**: Using torchtext to load the AG_NEWS dataset\n",
    "2. **Preprocessing**: Building vocabulary and tokenizing text\n",
    "3. **Model Architecture**: Neural network with embedding layer and fully connected layers\n",
    "4. **Training**: Gradient descent optimization with Adam optimizer\n",
    "5. **Evaluation**: Monitoring training progress and test accuracy\n",
    "6. **Prediction**: Making predictions on new text samples\n",
    "\n",
    "The model uses gradient descent to minimize cross-entropy loss and classify news articles into 4 categories. The embedding layer converts words to dense vectors, which are then averaged and passed through fully connected layers for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}